{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"E:/桌面/110/train_data.txt\"\n",
    "\n",
    "corpus = []\n",
    "file = codecs.open(filename,\"r\",\"utf-8\")\n",
    "for line in file.readlines():\n",
    "    corpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "stripcorpus = corpus.copy()\n",
    "onlycorpus = []\n",
    "for i in range(len(corpus)):\n",
    "    stripcorpus[i] = re.sub(\"@([\\s\\S]*?):\",\"\",corpus[i])  # 去除@ ...：\n",
    "    stripcorpus[i] = re.sub(\"\\[([\\S\\s]*?)\\]\",\"\",stripcorpus[i])  # [...]：\n",
    "    stripcorpus[i] = re.sub(\"@([\\s\\S]*?)\",\"\",stripcorpus[i])  # 去除@... \n",
    "    stripcorpus[i] = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\",\"\",stripcorpus[i])  # 去除标点及特殊符号\n",
    "    stripcorpus[i] = re.sub(\"[^\\u4e00-\\u9fa5]\",\"\",stripcorpus[i])  #  去除所有非汉字内容（英文数字）\n",
    "    stripcorpus[i] = re.sub(\"客户端\",\"\",stripcorpus[i])\n",
    "    stripcorpus[i] = re.sub(\"[A-Z]\",\"\",stripcorpus[i])\n",
    "\n",
    "for string in stripcorpus:\n",
    "    if(string == ''):\n",
    "        continue\n",
    "    else:\n",
    "        if(len(string)<5):\n",
    "            continue\n",
    "        else:\n",
    "            onlycorpus.append(string)\n",
    "cutcorpusiter = onlycorpus.copy()\n",
    "cutcorpus = onlycorpus.copy()\n",
    "cixingofword = []  # 储存分词后的词语对应的词性\n",
    "wordtocixing = []  # 储存分词后的词语\n",
    "for i in range(len(onlycorpus)):\n",
    "    cutcorpusiter[i] = pseg.cut(onlycorpus[i])\n",
    "    cutcorpus[i] = \"\"\n",
    "    for every in cutcorpusiter[i]:   \n",
    "        cutcorpus[i] = (cutcorpus[i] + \" \" + str(every.word)).strip()\n",
    "        cixingofword.append(every.flag)\n",
    "        wordtocixing.append(every.word)\n",
    "# 自己造一个{“词语”:“词性”}的字典，方便后续使用词性\n",
    "word2flagdict = {wordtocixing[i]:cixingofword[i] for i in range(len(wordtocixing))}\n",
    "word2flagdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()#该类会统计每个词语的tf-idf权值\n",
    "#第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(cutcorpus))\n",
    "#获取词袋模型中的所有词语  \n",
    "word = vectorizer.get_feature_names()\n",
    "#将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重\n",
    "weight = tfidf.toarray()\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordflagweight = [1 for i in range(len(word))]   #这个是词性系数，需要调整系数来看效果\n",
    "for i in range(len(word)):\n",
    "    if(word2flagdict[word[i]]==\"n\"):\n",
    "        wordflagweight[i] = 1.2\n",
    "    elif(word2flagdict[word[i]]==\"vn\"):\n",
    "        wordflagweight[i] = 1.1\n",
    "    elif(word2flagdict[word[i]]==\"m\"):\n",
    "        wordflagweight[i] = 0\n",
    "    else:        \n",
    "        continue\n",
    "\n",
    "import numpy as np\n",
    "wordflagweight = np.array(wordflagweight)\n",
    "newweight = weight.copy()\n",
    "for i in range(len(weight)):                \n",
    "    for j in range(len(word)):\n",
    "        newweight[i][j] = weight[i][j]*wordflagweight[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "DBS_clf = DBSCAN(eps=1, min_samples=6)\n",
    "DBS_clf.fit(newweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def DBS_Visualization(epsnumber,min_samplesnumber,X_weight):\n",
    "    DBS_clf = DBSCAN(eps=epsnumber,min_samples=min_samplesnumber)\n",
    "    DBS_clf.fit(X_weight)\n",
    "    labels_ = DBS_clf.labels_\n",
    "    X_reduction = PCA(n_components=(max(labels_)+1)).fit_transform(X_weight)  #这个weight是不需要改变的\n",
    "    X_reduction = TSNE(2).fit_transform(X_reduction)    #每次压缩的结果都是不一样的，因为n_components在变\n",
    "    signal = 0\n",
    "    noise = 0\n",
    "    xyclfweight = [[[],[]] for k in range(max(labels_)+2)] \n",
    "    for i in range(len(labels_)):\n",
    "        if(labels_[i]==-1):\n",
    "            noise += 1\n",
    "            xyclfweight[-1][0].append(X_reduction[i][0])\n",
    "            xyclfweight[-1][1].append(X_reduction[i][1])\n",
    "        else:    \n",
    "            for j in range(max(labels_)+1):\n",
    "                if(labels_[i]==j):\n",
    "                    signal += 1\n",
    "                    xyclfweight[j][0].append(X_reduction[i][0])\n",
    "                    xyclfweight[j][1].append(X_reduction[i][1])            \n",
    "    colors = ['red','blue','green','yellow','black','magenta'] * 3\n",
    "    for i in range(len(xyclfweight)-1):\n",
    "        plt.plot(xyclfweight[i][0],xyclfweight[i][1],color=colors[i])\n",
    "    plt.plot(xyclfweight[-1][0],xyclfweight[-1][1],color='#FFB6C1')\n",
    "    # 自适应坐标轴\n",
    "    plt.axis([min(X_reduction[:,0]),max(X_reduction[:,0]),min(X_reduction[:,1]),max(X_reduction[:,1])])\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "    print(\"分类数量（含噪声-1，粉色）= \"+str(max(labels_)+2),\"  \" + \"信噪比 = \"+str(signal/noise))  #包括噪声一共有多少类\n",
    "    print(\"eps = \"+str(epsnumber)+\"  \", \"min_sample = \"+str(min_samplesnumber))\n",
    "# for line in cutcorpus:\n",
    "#     print(line)\n",
    "list = []\n",
    "for i in range(0,len(cutcorpus)):\n",
    "    if DBS_clf.labels_[i] == 0:\n",
    "        print(\"0:\",corpus[i])\n",
    "        \n",
    "for i in range(0,len(cutcorpus)):\n",
    "    if DBS_clf.labels_[i] == -1:\n",
    "        print(\"-1:\",corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
